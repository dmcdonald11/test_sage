# Crawl Process Architecture Documentation

## Table of Contents
- [Overview](#overview)
- [Architecture Flow](#architecture-flow)
- [Core Components](#core-components)
- [Crawl Strategies](#crawl-strategies)
- [Process Flows](#process-flows)
- [Code Examples](#code-examples)
- [Configuration](#configuration)
- [Progress Tracking](#progress-tracking)

---

## Overview

The ArchonDM crawling system is a sophisticated web crawling architecture built on **Crawl4AI v0.6.2** that intelligently extracts, processes, and stores web content in a knowledge base. The system uses a strategy pattern to handle different types of content (single pages, batch URLs, recursive crawls, sitemaps) and provides real-time progress tracking via HTTP polling.

### Key Features
- **Intelligent URL Detection**: Automatically detects content type (sitemap, text file, markdown, link collection, binary)
- **Multiple Crawl Strategies**: Single page, batch, recursive, and sitemap strategies
- **Progress Tracking**: Real-time progress updates via HTTP polling
- **Code Extraction**: Automatically extracts and indexes code examples
- **Cancellation Support**: Graceful cancellation of long-running crawl operations
- **Memory-Adaptive Processing**: Automatically throttles based on system memory
- **Supabase Integration**: Stores documents with vector embeddings for RAG

---

## Architecture Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          CRAWL REQUEST                                   â”‚
â”‚                    POST /api/knowledge-items/crawl                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   knowledge_api.py            â”‚
                â”‚   - Validate URL & API key    â”‚
                â”‚   - Create progress_id        â”‚
                â”‚   - Initialize ProgressTrackerâ”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  CrawlerManager              â”‚
                â”‚  - Singleton pattern         â”‚
                â”‚  - Initializes AsyncWebCrawlerâ”‚
                â”‚  - Configures Chromium browserâ”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  CrawlingService (Orchestrator)      â”‚
                â”‚  - Analyzes URL type                 â”‚
                â”‚  - Selects appropriate strategy      â”‚
                â”‚  - Manages progress tracking         â”‚
                â”‚  - Handles cancellation              â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                               â–¼           â–¼          â–¼          â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚Single Page   â”‚ â”‚ Batch   â”‚ â”‚Recursive â”‚ â”‚Sitemap  â”‚
                    â”‚Strategy      â”‚ â”‚Strategy â”‚ â”‚Strategy  â”‚ â”‚Strategy â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                           â”‚              â”‚           â”‚            â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚
                                          â–¼
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚  Crawl Results (Raw Content)         â”‚
                           â”‚  [{url, markdown, html, links}...]   â”‚
                           â”‚                                      â”‚
                           â”‚  â€¢ markdown: Crawl4AI parsed text   â”‚
                           â”‚  â€¢ html: Raw HTML for code extract   â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                                               â”‚
                    â–¼                                               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ TEXT PROCESSING FLOW              â”‚         â”‚ CODE EXTRACTION FLOW           â”‚
    â”‚ (DocumentStorageOperations)       â”‚         â”‚ (CodeExtractionService)        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                                               â”‚
                    â–¼                                               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 1. PARSE & CHUNK MARKDOWN         â”‚         â”‚ 1. PARSE HTML CONTENT          â”‚
    â”‚    â€¢ Split into 5000 char chunks  â”‚         â”‚    â€¢ Find <pre><code> blocks   â”‚
    â”‚    â€¢ Smart chunking at boundaries â”‚         â”‚    â€¢ Parse markdown ```blocks  â”‚
    â”‚    â€¢ Preserve context             â”‚         â”‚    â€¢ Detect language           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                                               â”‚
                 â–¼                                               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 2. CREATE SOURCE RECORDS          â”‚         â”‚ 2. FILTER CODE BLOCKS          â”‚
    â”‚    â€¢ Generate unique source_id    â”‚         â”‚    â€¢ Min length: 1000 chars    â”‚
    â”‚    â€¢ Extract display name         â”‚         â”‚    â€¢ Remove duplicates         â”‚
    â”‚    â€¢ AI-generate summary          â”‚         â”‚    â€¢ Validate syntax           â”‚
    â”‚    â€¢ Store in archon_sources      â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
                 â”‚                                               â–¼
                 â–¼                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚ 3. AI SUMMARY GENERATION       â”‚
    â”‚ 3. GENERATE EMBEDDINGS            â”‚         â”‚    â€¢ ThreadingService (4-16    â”‚
    â”‚    â€¢ Create vectors for each chunkâ”‚         â”‚      workers, CPU-adaptive)    â”‚
    â”‚    â€¢ Batch processing (25/batch)  â”‚         â”‚    â€¢ Rate limiting (200k TPM)  â”‚
    â”‚    â€¢ Provider: OpenAI/Google/     â”‚         â”‚    â€¢ Generate code summaries   â”‚
    â”‚      Ollama                       â”‚         â”‚    â€¢ Extract purpose & usage   â”‚
    â”‚    â€¢ Parallel processing enabled  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
                 â”‚                                               â–¼
                 â–¼                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚ 4. STORE CODE EXAMPLES         â”‚
    â”‚ 4. STORE IN SUPABASE              â”‚         â”‚    â€¢ Save to code_examples     â”‚
    â”‚    â€¢ archon_documents table       â”‚         â”‚      table                     â”‚
    â”‚    â€¢ Vector embeddings            â”‚         â”‚    â€¢ Include: language, summaryâ”‚
    â”‚    â€¢ Metadata: url, source_id,    â”‚         â”‚    â€¢ Link to source_id         â”‚
    â”‚      tags, word_count             â”‚         â”‚    â€¢ Enable code search        â”‚
    â”‚    â€¢ Enable semantic search       â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
                 â”‚                                               â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚  Crawl Complete     â”‚
                          â”‚  Progress: 100%     â”‚
                          â”‚                     â”‚
                          â”‚  Results:           â”‚
                          â”‚  â€¢ Documents stored â”‚
                          â”‚  â€¢ Code indexed     â”‚
                          â”‚  â€¢ RAG ready        â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Core Components

### 1. CrawlerManager (`python/src/server/services/crawler_manager.py`)

**Purpose**: Manages the global Crawl4AI crawler instance as a singleton.

**Key Responsibilities**:
- Initialize `AsyncWebCrawler` with optimized browser configuration
- Configure Chromium with performance optimizations (disable images, GPU, etc.)
- Provide thread-safe access to the crawler instance
- Handle cleanup on shutdown

**Code Location**: `python/src/server/services/crawler_manager.py`

**Key Configuration**:
```python
browser_config = BrowserConfig(
    headless=True,
    verbose=False,
    viewport_width=1920,
    viewport_height=1080,
    browser_type="chromium",
    extra_args=[
        "--disable-images",  # Skip image loading
        "--disable-gpu",
        "--no-sandbox",
        "--disable-setuid-sandbox",
        # ... more optimizations
    ]
)
```

### 2. CrawlingService (`python/src/server/services/crawling/crawling_service.py`)

**Purpose**: Main orchestrator that coordinates the entire crawl process.

**Key Responsibilities**:
- Analyze URL and determine crawl type
- Select and execute appropriate crawl strategy
- Track progress across all stages
- Handle cancellation requests
- Coordinate document storage and code extraction

**Key Methods**:
- `orchestrate_crawl()`: Main entry point for crawl operations
- `_detect_crawl_type()`: Determines URL type (sitemap, text, markdown, etc.)
- `cancel()`: Gracefully cancel ongoing operations
- `is_cancelled()`: Check cancellation status

### 3. URLHandler (`python/src/server/services/crawling/helpers/url_handler.py`)

**Purpose**: Handles URL validation, transformation, and classification.

**Key Methods**:
- `is_sitemap(url)`: Detect sitemap.xml files
- `is_txt(url)`: Detect .txt files
- `is_markdown(url)`: Detect .md/.mdx/.markdown files
- `is_binary_file(url)`: Skip binary files (zip, pdf, images, etc.)
- `is_link_collection_file(url, content)`: Detect llms.txt-style link collections
- `transform_github_url(url)`: Convert GitHub file URLs to raw.githubusercontent.com
- `generate_unique_source_id(url)`: Generate hash-based unique IDs
- `extract_display_name(url)`: Create human-readable names for sources
- `extract_markdown_links(content, base_url)`: Extract links from markdown/text

### 4. DocumentStorageOperations (`python/src/server/services/crawling/document_storage_operations.py`)

**Purpose**: Handles document processing, chunking, and storage.

**Key Responsibilities**:
- Chunk documents into 5000-character segments
- Create/update source records in database
- Generate embeddings for each chunk
- Store documents in Supabase with metadata
- Support parallel batch processing

**Key Methods**:
- `process_and_store_documents()`: Main document processing pipeline
- `_create_source_records()`: Create source entries in database

### 5. CodeExtractionService (`python/src/server/services/crawling/code_extraction_service.py`)

**Purpose**: Extract code examples from HTML and generate AI summaries.

**Key Responsibilities**:
- Extract code blocks from HTML content
- Filter by minimum length (default: 1000 characters)
- Generate AI-powered summaries for each code block
- Store in `code_examples` table with metadata

### 6. ProgressMapper (`python/src/server/services/crawling/progress_mapper.py`)

**Purpose**: Maps sub-stage progress to overall progress percentages.

**Stage Ranges**:
```python
STAGE_RANGES = {
    "analyzing": (1, 3),          # URL analysis
    "crawling": (3, 15),          # Web crawling
    "processing": (15, 20),       # Content processing
    "source_creation": (20, 25),  # Database operations
    "document_storage": (25, 40), # Embeddings generation
    "code_extraction": (40, 90),  # Code extraction + summaries
    "finalization": (90, 100),    # Final cleanup
}
```

---

## Crawl Strategies

### 1. Single Page Strategy (`strategies/single_page.py`)

**When Used**: Single URL that's not a sitemap, link collection, or binary file.

**Process**:
1. Transform URL (e.g., GitHub â†’ raw.githubusercontent.com)
2. Detect if documentation site (special handling)
3. Configure Crawl4AI with appropriate settings
4. Crawl with retry logic (3 attempts with exponential backoff)
5. Return markdown and HTML content

**Configuration for Documentation Sites**:
```python
CrawlerRunConfig(
    wait_for=wait_selector,        # e.g., '.markdown, article'
    wait_until='domcontentloaded',
    page_timeout=30000,
    delay_before_return_html=0.5,
    scan_full_page=True,
    remove_overlay_elements=True,
    process_iframes=True
)
```

### 2. Batch Strategy (`strategies/batch.py`)

**When Used**: Multiple URLs (from sitemap or link collection).

**Process**:
1. Load batch size and concurrency settings from database
2. Initialize `MemoryAdaptiveDispatcher` for memory management
3. Process URLs in batches (default: 50 URLs per batch)
4. Use `crawler.arun_many()` for parallel crawling
5. Stream results as they complete
6. Report progress for each batch

**Key Features**:
- Parallel crawling with configurable concurrency
- Memory-adaptive throttling
- Streaming results for incremental progress
- Cancellation support between batches

### 3. Recursive Strategy (`strategies/recursive.py`)

**When Used**: Crawl a website by following internal links to a specified depth.

**Process**:
1. Start with initial URL(s)
2. For each depth level (default: 3):
   - Crawl all URLs at current depth
   - Extract internal links from results
   - Filter out binary files and visited URLs
   - Add new URLs to next depth queue
3. Continue until max depth reached or no new URLs

**Key Features**:
- Depth-limited crawling
- Automatic link extraction
- De-duplication of URLs
- Progress reporting per depth level
- Cancellation support at batch and depth boundaries

### 4. Sitemap Strategy (`strategies/sitemap.py`)

**When Used**: URL ends with `sitemap.xml`.

**Process**:
1. Fetch sitemap XML file
2. Parse with `ElementTree`
3. Extract all `<loc>` elements
4. Return list of URLs for batch crawling

**Note**: After parsing, the batch strategy is used to crawl all URLs.

---

## Process Flows

### Flow 1: Single Page Crawl

```
User Request â†’ API Validation â†’ Get Crawler â†’ Single Page Strategy
                                                      â†“
                                            Crawl with retries
                                                      â†“
                                            Return {url, markdown, html}
                                                      â†“
                                            Chunk content (5000 chars)
                                                      â†“
                                            Create source record
                                                      â†“
                                            Generate embeddings
                                                      â†“
                                            Store in Supabase
                                                      â†“
                                            Extract code examples
                                                      â†“
                                            Complete (100%)
```

### Flow 2: Sitemap Crawl

```
User Request â†’ API Validation â†’ Detect sitemap.xml â†’ Sitemap Strategy
                                                             â†“
                                                   Parse XML for URLs
                                                             â†“
                                                   Batch Strategy
                                                             â†“
                                          Process in batches of 50 URLs
                                                             â†“
                                          Parallel crawl with arun_many()
                                                             â†“
                                          Stream results incrementally
                                                             â†“
                                          DocumentStorageOperations
                                          (same as single page)
```

### Flow 3: Recursive Crawl

```
User Request â†’ API Validation â†’ Detect normal webpage â†’ Recursive Strategy
                                                              â†“
                                                    Crawl starting URLs
                                                              â†“
                                                    Extract internal links
                                                              â†“
                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                              â–¼                            â–¼
                                        Depth 1 URLs                  Depth 2 URLs
                                              â†“                            â†“
                                        Batch crawl                  Batch crawl
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                           â–¼
                                                     Depth 3 URLs
                                                           â†“
                                                     Batch crawl
                                                           â†“
                                              DocumentStorageOperations
                                              (same as single page)
```

### Flow 4: Link Collection (llms.txt) Crawl

```
User Request â†’ API Validation â†’ Detect llms.txt â†’ Single Page Strategy
                                                          â†“
                                                Fetch text file
                                                          â†“
                                                Extract URLs from content
                                                          â†“
                                                Batch Strategy
                                                          â†“
                                        Crawl all discovered URLs
                                                          â†“
                                        DocumentStorageOperations
                                        (same as single page)
```

---

## Code Examples

### Example 1: Starting a Crawl from API

```python
# python/src/server/api_routes/knowledge_api.py

@router.post("/knowledge-items/crawl")
async def crawl_knowledge_item(request: KnowledgeItemRequest):
    # Generate unique progress ID
    progress_id = str(uuid.uuid4())
    
    # Initialize progress tracker
    tracker = ProgressTracker(progress_id, operation_type="crawl")
    await tracker.start({
        "url": str(request.url),
        "crawl_type": "normal",
        "progress": 0,
        "log": f"Starting crawl for {request.url}"
    })
    
    # Start background task
    asyncio.create_task(_perform_crawl_with_progress(progress_id, request, tracker))
    
    return {
        "success": True,
        "progressId": progress_id,
        "message": "Crawling started",
        "estimatedDuration": "3-5 minutes"
    }
```

### Example 2: Orchestrating a Crawl

```python
# python/src/server/services/crawling/crawling_service.py

async def orchestrate_crawl(self, request: dict[str, Any]) -> dict[str, Any]:
    """Main orchestration method for crawling operations."""
    
    # Step 1: Analyze URL
    crawl_type = await self._detect_crawl_type(request['url'])
    
    # Step 2: Execute appropriate strategy
    if crawl_type == "single_page":
        results = await self._crawl_single_page(request)
    elif crawl_type == "sitemap":
        results = await self._crawl_sitemap(request)
    elif crawl_type == "recursive":
        results = await self._crawl_recursive(request)
    elif crawl_type == "text_file":
        results = await self._crawl_text_file(request)
    
    # Step 3: Process and store documents
    storage_result = await self.doc_storage_ops.process_and_store_documents(
        crawl_results=results,
        request=request,
        crawl_type=crawl_type,
        original_source_id=source_id,
        progress_callback=self._create_progress_callback("document_storage")
    )
    
    # Step 4: Extract code examples (if enabled)
    if request.get("extract_code_examples", True):
        code_count = await self.doc_storage_ops.extract_and_store_code_examples(
            crawl_results=results,
            url_to_full_document=storage_result["url_to_full_document"],
            source_id=source_id,
            progress_callback=self._create_progress_callback("code_extraction")
        )
    
    return {"success": True, "chunks_stored": storage_result["chunks_stored"]}
```

### Example 3: Batch Crawling with Progress

```python
# python/src/server/services/crawling/strategies/batch.py

async def crawl_batch_with_progress(
    self,
    urls: list[str],
    progress_callback: Callable[..., Awaitable[None]] | None = None
) -> list[dict[str, Any]]:
    """Batch crawl URLs with progress reporting."""
    
    batch_size = 50
    total_urls = len(urls)
    successful_results = []
    
    for i in range(0, total_urls, batch_size):
        batch_urls = urls[i : i + batch_size]
        
        # Report progress
        progress_percentage = int((i / total_urls) * 100)
        await progress_callback(
            "crawling",
            progress_percentage,
            f"Processing batch {i + 1}-{min(i + batch_size, total_urls)} of {total_urls}",
            total_pages=total_urls,
            processed_pages=i
        )
        
        # Crawl batch in parallel
        batch_results = await self.crawler.arun_many(
            urls=batch_urls,
            config=crawl_config,
            dispatcher=dispatcher
        )
        
        # Stream results
        async for result in batch_results:
            if result.success and result.markdown:
                successful_results.append({
                    "url": result.url,
                    "markdown": result.markdown,
                    "html": result.html
                })
    
    return successful_results
```

### Example 4: URL Type Detection

```python
# python/src/server/services/crawling/helpers/url_handler.py

class URLHandler:
    @staticmethod
    def is_link_collection_file(url: str, content: Optional[str] = None) -> bool:
        """Check if file is a link collection like llms.txt."""
        parsed = urlparse(url)
        filename = parsed.path.split('/')[-1].lower()
        
        # Check filename patterns
        link_collection_patterns = [
            'llms.txt', 'links.txt', 'resources.txt',
            'llms.md', 'links.md', 'resources.md'
        ]
        
        if filename in link_collection_patterns:
            return True
        
        # Content-based detection
        if content and 'full' not in filename:
            extracted_links = URLHandler.extract_markdown_links(content, url)
            total_links = len(extracted_links)
            content_length = len(content.strip())
            
            if content_length > 0:
                link_density = (total_links * 100) / content_length
                # If more than 2% of content is links
                if link_density > 2.0 and total_links > 3:
                    return True
        
        return False
```

---

## Configuration

### Database Settings (Stored in Supabase)

Configuration settings are stored in the `credentials` table under category `rag_strategy`:

| Setting                   | Default | Description                                    |
|---------------------------|---------|------------------------------------------------|
| `CRAWL_BATCH_SIZE`        | 50      | Number of URLs to crawl per batch             |
| `CRAWL_MAX_CONCURRENT`    | 10      | Max parallel crawls within a single operation |
| `MEMORY_THRESHOLD_PERCENT`| 80      | Memory usage threshold for throttling (%)     |
| `DISPATCHER_CHECK_INTERVAL`| 0.5    | How often to check memory (seconds)           |
| `CRAWL_WAIT_STRATEGY`     | domcontentloaded | Playwright wait strategy         |
| `CRAWL_PAGE_TIMEOUT`      | 30000   | Page load timeout (milliseconds)              |
| `CRAWL_DELAY_BEFORE_HTML` | 1.0     | Delay after page load (seconds)               |
| `CODE_BLOCK_MIN_LENGTH`   | 1000    | Minimum code block size for extraction        |

### Environment Variables

```bash
# Crawl4AI Configuration
CONCURRENT_CRAWL_LIMIT=3           # Server-level crawl concurrency limit

# Supabase
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_SERVICE_KEY=your-service-key

# Embedding Provider
EMBEDDING_PROVIDER=openai          # openai, google, or ollama
OPENAI_API_KEY=your-key
```

### Browser Configuration

Located in `CrawlerManager.initialize()`:

```python
browser_config = BrowserConfig(
    headless=True,
    verbose=False,
    viewport_width=1920,
    viewport_height=1080,
    user_agent="Mozilla/5.0 ...",
    browser_type="chromium",
    extra_args=[
        "--disable-images",         # Skip images for speed
        "--disable-gpu",
        "--no-sandbox",
        "--disable-setuid-sandbox",
        "--disable-web-security",
        # ... more performance optimizations
    ]
)
```

---

## Progress Tracking

### Progress Architecture

The system uses HTTP polling for real-time progress updates:

```
Frontend (React) â†’ Poll /api/progress/{progress_id} every 500ms
                          â†“
                   ProgressTracker (in-memory)
                          â†“
                   Crawling Service updates
```

### Progress Stages

| Stage              | Range    | Description                              |
|--------------------|----------|------------------------------------------|
| `analyzing`        | 1-3%     | URL analysis and type detection          |
| `crawling`         | 3-15%    | Web crawling (varies by depth/count)     |
| `processing`       | 15-20%   | Content processing and chunking          |
| `source_creation`  | 20-25%   | Creating database source records         |
| `document_storage` | 25-40%   | Generating embeddings and storing chunks |
| `code_extraction`  | 40-90%   | Extracting code + AI summaries           |
| `finalization`     | 90-100%  | Final cleanup and completion             |

### Progress Update Example

```python
# In any crawl strategy
async def report_progress(progress_val: int, message: str, **kwargs):
    if progress_callback:
        await progress_callback(
            "crawling",              # Current stage
            progress_val,            # 0-100 within stage
            message,                 # User-friendly message
            total_pages=total_urls,  # Total URLs to crawl
            processed_pages=current  # URLs crawled so far
        )
```

### Cancellation Support

Users can cancel long-running crawls:

```python
# API endpoint
@router.delete("/knowledge-items/crawl/{progress_id}")
async def cancel_crawl(progress_id: str):
    # Cancel the orchestration service
    orchestration = get_active_orchestration(progress_id)
    if orchestration:
        orchestration.cancel()
    
    # Cancel the task
    task = active_crawl_tasks.get(progress_id)
    if task:
        task.cancel()
```

Each strategy checks for cancellation between batches/pages:

```python
if cancellation_check:
    try:
        cancellation_check()
    except asyncio.CancelledError:
        # Cleanup and exit gracefully
        await report_progress(99, "Crawl cancelled", status="cancelled")
        return partial_results
```

---

---

## Multithreading & Concurrent Crawl Management

The ArchonDM crawling system implements a sophisticated multi-level concurrency architecture that allows efficient parallel processing of multiple websites while protecting server resources.

### Concurrency Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LEVEL 1: Server-Level Concurrency                     â”‚
â”‚                  (Limits Total Number of Crawl Operations)               â”‚
â”‚                                                                           â”‚
â”‚   User A: Crawl site1.com    User B: Crawl site2.com    User C: Queued  â”‚
â”‚        [RUNNING]                  [RUNNING]                 [WAITING]    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ Progress:   â”‚            â”‚ Progress:   â”‚          â”‚  Semaphore   â”‚  â”‚
â”‚   â”‚   45%       â”‚            â”‚   78%       â”‚          â”‚  Limit: 3    â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚          â”‚                          â”‚                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                          â”‚
           â–¼                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LEVEL 2: Within-Crawl Parallel Page Processing              â”‚
â”‚                   (Each crawl processes multiple pages)                  â”‚
â”‚                                                                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚  Crawl site1.com - Batch Strategy                           â”‚       â”‚
â”‚   â”‚                                                              â”‚       â”‚
â”‚   â”‚  Page 1   Page 2   Page 3   Page 4   Page 5  ... Page 10   â”‚       â”‚
â”‚   â”‚  [DONE]   [DONE]   [CRAWL]  [CRAWL]  [WAIT]      [WAIT]    â”‚       â”‚
â”‚   â”‚                                                              â”‚       â”‚
â”‚   â”‚  MemoryAdaptiveDispatcher: 10 concurrent pages              â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           LEVEL 3: Adaptive Worker Pool (Threading Service)             â”‚
â”‚                 (CPU-intensive operations like embeddings)               â”‚
â”‚                                                                           â”‚
â”‚   Worker 1    Worker 2    Worker 3    Worker 4    ... Worker N          â”‚
â”‚   [Embed]     [Embed]     [Code AI]   [IDLE]                            â”‚
â”‚                                                                           â”‚
â”‚   Dynamically adjusts based on CPU/Memory (4-16 workers)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Level 1: Server-Level Concurrency Control

**Purpose**: Prevent server overload when multiple users initiate crawls simultaneously.

**Implementation**: `python/src/server/api_routes/knowledge_api.py`

**Key Code**:
```python
# Hardcoded limit to protect server resources
CONCURRENT_CRAWL_LIMIT = 3  # Max simultaneous crawl operations

# Create asyncio Semaphore
crawl_semaphore = asyncio.Semaphore(CONCURRENT_CRAWL_LIMIT)

# Track active crawl tasks
active_crawl_tasks: dict[str, asyncio.Task] = {}

async def _perform_crawl_with_progress(
    progress_id: str, request: KnowledgeItemRequest, tracker
):
    """Perform crawl operation with semaphore protection."""
    # Acquire semaphore - blocks if 3 crawls are already running
    async with crawl_semaphore:
        logger.info(f"Acquired crawl semaphore | progress_id={progress_id}")
        
        # Get crawler and start operation
        crawler = await get_crawler()
        orchestration_service = CrawlingService(crawler, supabase_client)
        orchestration_service.set_progress_id(progress_id)
        
        # Orchestrate the crawl
        result = await orchestration_service.orchestrate_crawl(request_dict)
        
        # Store task for cancellation support
        crawl_task = result.get("task")
        if crawl_task:
            active_crawl_tasks[progress_id] = crawl_task
```

**Behavior**:
- **Max 3 concurrent crawl operations** (configurable via `CONCURRENT_CRAWL_LIMIT`)
- 4th crawl request waits until one of the first 3 completes
- Each crawl gets full access to within-crawl parallelism
- Semaphore automatically releases when crawl completes (via context manager)

### Level 2: Within-Crawl Parallel Page Processing

**Purpose**: Process multiple pages in parallel within a single crawl operation.

**Implementation**: Uses Crawl4AI's `MemoryAdaptiveDispatcher` in batch and recursive strategies.

**Configuration** (from database `credentials` table):
```python
CRAWL_MAX_CONCURRENT = 10  # Max pages to crawl in parallel
MEMORY_THRESHOLD_PERCENT = 80  # Throttle if memory exceeds 80%
```

**Example: Batch Strategy**:
```python
# python/src/server/services/crawling/strategies/batch.py

async def crawl_batch_with_progress(
    self,
    urls: list[str],
    max_concurrent: int = 10,
    progress_callback: Callable | None = None
) -> list[dict[str, Any]]:
    """Crawl multiple URLs in parallel with memory management."""
    
    # Load settings from database
    settings = await credential_service.get_credentials_by_category("rag_strategy")
    max_concurrent = int(settings.get("CRAWL_MAX_CONCURRENT", "10"))
    memory_threshold = float(settings.get("MEMORY_THRESHOLD_PERCENT", "80"))
    
    # Create memory-adaptive dispatcher
    dispatcher = MemoryAdaptiveDispatcher(
        memory_threshold_percent=memory_threshold,
        check_interval=0.5,  # Check memory every 0.5 seconds
        max_session_permit=max_concurrent  # Max parallel crawls
    )
    
    # Crawl in batches of 50 URLs
    batch_size = 50
    for i in range(0, len(urls), batch_size):
        batch_urls = urls[i : i + batch_size]
        
        # Crawl this batch with up to 10 pages in parallel
        batch_results = await self.crawler.arun_many(
            urls=batch_urls,
            config=crawl_config,
            dispatcher=dispatcher  # Handles parallel execution
        )
        
        # Stream results as they complete
        async for result in batch_results:
            if result.success:
                successful_results.append(result)
            
            # Report progress
            await progress_callback(
                "crawling",
                int((processed / total) * 100),
                f"Crawled {processed}/{total} pages"
            )
    
    return successful_results
```

**Memory-Adaptive Behavior**:
- Monitors system memory usage in real-time
- Automatically reduces parallel crawls if memory exceeds threshold
- Resumes full parallelism when memory drops below threshold
- Prevents out-of-memory crashes during large crawls

### Level 3: Adaptive Worker Pool (Threading Service)

**Purpose**: Efficiently handle CPU-intensive operations (embeddings, AI summaries) and I/O-bound tasks.

**Implementation**: `python/src/server/services/threading_service.py`

**Key Features**:
1. **Multiple Processing Modes**:
   - `CPU_INTENSIVE`: AI summaries, embeddings (4-16 workers, uses CPU count)
   - `IO_BOUND`: Database operations, file I/O (8-32 workers)
   - `NETWORK_BOUND`: External API calls (4-16 workers)

2. **Dynamic Worker Adjustment**:
   - Monitors CPU and memory in real-time
   - Reduces workers when resources are constrained
   - Increases workers when resources are available

3. **Rate Limiting**:
   - Token bucket algorithm for API rate limiting
   - Prevents API quota exhaustion
   - Automatic backoff and retry

**Example Usage**:
```python
# python/src/server/services/threading_service.py

class ThreadingService:
    """Main threading service for adaptive concurrency."""
    
    async def batch_process(
        self,
        items: list[Any],
        process_func: Callable,
        mode: ProcessingMode = ProcessingMode.CPU_INTENSIVE,
        progress_callback: Callable | None = None
    ) -> list[Any]:
        """Process items with adaptive worker pool."""
        
        # Calculate optimal workers based on system load
        optimal_workers = self.memory_dispatcher.calculate_optimal_workers(mode)
        semaphore = asyncio.Semaphore(optimal_workers)
        
        logger.info(
            f"Starting adaptive processing: {len(items)} items, "
            f"{optimal_workers} workers, mode={mode}"
        )
        
        async def process_single(item: Any, index: int) -> Any:
            async with semaphore:
                # For CPU-intensive, run in thread pool
                if mode == ProcessingMode.CPU_INTENSIVE:
                    loop = asyncio.get_event_loop()
                    result = await loop.run_in_executor(None, process_func, item)
                else:
                    # Run directly if async
                    result = await process_func(item)
                
                # Report progress
                if progress_callback:
                    await progress_callback({
                        "type": "worker_completed",
                        "completed": index + 1,
                        "total": len(items)
                    })
                
                return result
        
        # Execute all items with controlled concurrency
        tasks = [process_single(item, idx) for idx, item in enumerate(items)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        return [r for r in results if not isinstance(r, Exception)]
```

**Worker Calculation Logic**:
```python
def calculate_optimal_workers(self, mode: ProcessingMode) -> int:
    """Calculate optimal worker count based on system resources."""
    metrics = self.get_system_metrics()
    
    # Base worker count depends on mode
    if mode == ProcessingMode.CPU_INTENSIVE:
        base = min(4, psutil.cpu_count())
    elif mode == ProcessingMode.IO_BOUND:
        base = 8  # 2x for I/O operations
    else:
        base = 4
    
    # Adjust based on system load
    if metrics.memory_percent > 80:
        workers = max(1, base // 2)  # Reduce by 50%
    elif metrics.cpu_percent > 90:
        workers = max(1, base // 2)  # Reduce by 50%
    elif metrics.memory_percent < 50 and metrics.cpu_percent < 50:
        workers = min(16, base * 2)  # Increase up to 2x
    else:
        workers = base
    
    return workers
```

### Complete Multi-Site Crawl Example

Here's a complete example showing how to crawl multiple websites concurrently:

```python
"""
Example: Crawl multiple documentation sites concurrently
"""

import asyncio
from python.src.server.api_routes.knowledge_api import crawl_knowledge_item
from python.src.server.services.crawling import CrawlingService
from python.src.server.services.crawler_manager import get_crawler
from python.src.server.utils import get_supabase_client

async def crawl_multiple_sites(site_urls: list[str]) -> dict:
    """
    Crawl multiple sites concurrently with automatic queuing.
    
    Args:
        site_urls: List of website URLs to crawl
    
    Returns:
        Dict with progress IDs and status for each site
    """
    results = {
        "started": [],
        "queued": [],
        "total": len(site_urls)
    }
    
    async def crawl_single_site(url: str, index: int):
        """Crawl a single site."""
        try:
            # Create crawl request
            request = KnowledgeItemRequest(
                url=url,
                knowledge_type="documentation",
                tags=["auto-crawl"],
                max_depth=2,
                extract_code_examples=True
            )
            
            # This will automatically queue if 3 crawls are running
            response = await crawl_knowledge_item(request)
            
            print(f"âœ… Site {index + 1}/{len(site_urls)}: {url}")
            print(f"   Progress ID: {response['progressId']}")
            print(f"   Status: {response['message']}")
            
            results["started"].append({
                "url": url,
                "progress_id": response["progressId"],
                "index": index
            })
            
        except Exception as e:
            print(f"âŒ Site {index + 1}/{len(site_urls)} failed: {url}")
            print(f"   Error: {str(e)}")
    
    # Start all crawls concurrently
    # The semaphore will automatically queue excess requests
    crawl_tasks = [
        crawl_single_site(url, idx) 
        for idx, url in enumerate(site_urls)
    ]
    
    # Wait for all to start (some may be queued)
    await asyncio.gather(*crawl_tasks, return_exceptions=True)
    
    return results

# Example usage
async def main():
    sites_to_crawl = [
        "https://docs.python.org/3/",
        "https://fastapi.tiangolo.com/",
        "https://docs.pydantic.dev/",
        "https://www.supabase.com/docs",
        "https://platform.openai.com/docs",
    ]
    
    print(f"ğŸš€ Starting crawl of {len(sites_to_crawl)} sites...")
    print(f"   Server limit: 3 concurrent crawls")
    print(f"   Sites 1-3 will start immediately")
    print(f"   Sites 4-5 will queue and start as others complete\n")
    
    results = await crawl_multiple_sites(sites_to_crawl)
    
    print("\n" + "="*60)
    print(f"ğŸ“Š Crawl Summary:")
    print(f"   Total sites: {results['total']}")
    print(f"   Started/Queued: {len(results['started'])}")
    print("="*60)

# Run the example
if __name__ == "__main__":
    asyncio.run(main())
```

**Output**:
```
ğŸš€ Starting crawl of 5 sites...
   Server limit: 3 concurrent crawls
   Sites 1-3 will start immediately
   Sites 4-5 will queue and start as others complete

âœ… Site 1/5: https://docs.python.org/3/
   Progress ID: a1b2c3d4-e5f6-7890-abcd-ef1234567890
   Status: Crawling started

âœ… Site 2/5: https://fastapi.tiangolo.com/
   Progress ID: b2c3d4e5-f6a7-8901-bcde-f12345678901
   Status: Crawling started

âœ… Site 3/5: https://docs.pydantic.dev/
   Progress ID: c3d4e5f6-a7b8-9012-cdef-123456789012
   Status: Crawling started

[Site 4 waits until one of the first 3 completes...]

âœ… Site 4/5: https://www.supabase.com/docs
   Progress ID: d4e5f6a7-b8c9-0123-def1-234567890123
   Status: Crawling started

âœ… Site 5/5: https://platform.openai.com/docs
   Progress ID: e5f6a7b8-c9d0-1234-ef12-345678901234
   Status: Crawling started

============================================================
ğŸ“Š Crawl Summary:
   Total sites: 5
   Started/Queued: 5
============================================================
```

### Monitoring Active Crawls

You can monitor and manage active crawls:

```python
from python.src.server.api_routes.knowledge_api import active_crawl_tasks

def get_active_crawls() -> dict:
    """Get status of all active crawls."""
    return {
        "active_count": len(active_crawl_tasks),
        "max_concurrent": CONCURRENT_CRAWL_LIMIT,
        "available_slots": CONCURRENT_CRAWL_LIMIT - len(active_crawl_tasks),
        "crawls": [
            {
                "progress_id": progress_id,
                "task_name": task.get_name(),
                "done": task.done()
            }
            for progress_id, task in active_crawl_tasks.items()
        ]
    }

# Cancel a specific crawl
async def cancel_crawl(progress_id: str):
    """Cancel a running crawl operation."""
    task = active_crawl_tasks.get(progress_id)
    if task and not task.done():
        task.cancel()
        print(f"Cancelled crawl: {progress_id}")
    else:
        print(f"Crawl not found or already completed: {progress_id}")
```

### Configuration Summary

| Level | Setting | Location | Default | Description |
|-------|---------|----------|---------|-------------|
| **Server** | `CONCURRENT_CRAWL_LIMIT` | `knowledge_api.py` | 3 | Max simultaneous crawl operations |
| **Crawl** | `CRAWL_MAX_CONCURRENT` | Database (credentials) | 10 | Max pages crawled in parallel |
| **Crawl** | `CRAWL_BATCH_SIZE` | Database (credentials) | 50 | URLs per batch |
| **Crawl** | `MEMORY_THRESHOLD_PERCENT` | Database (credentials) | 80 | Memory throttle threshold |
| **Threading** | `base_workers` | `ThreadingConfig` | 4 | Base worker count |
| **Threading** | `max_workers` | `ThreadingConfig` | 16 | Maximum worker count |
| **Threading** | `memory_threshold` | `ThreadingConfig` | 0.8 | Memory limit (80%) |
| **Threading** | `cpu_threshold` | `ThreadingConfig` | 0.9 | CPU limit (90%) |

### Performance Characteristics

**Typical Performance** (on 8-core, 16GB RAM server):

- **Single Site Crawl**: 50-100 pages in 2-3 minutes
- **Concurrent 3 Sites**: 150-300 pages in 3-5 minutes (minimal slowdown)
- **Memory Usage**: 2-4GB per active crawl operation
- **CPU Usage**: 50-70% during active crawling, 80-90% during embedding generation

**Scalability**:
- Can handle 3 simultaneous large crawls (1000+ pages each)
- Automatically throttles if memory exceeds 80%
- Each crawl processes 10 pages in parallel by default
- Threading service adjusts 4-16 workers based on system load

---

## Summary

The ArchonDM crawl process is a robust, production-ready system that:

1. **Intelligently analyzes URLs** to determine the best crawl strategy
2. **Executes crawls efficiently** using multi-level parallel processing
3. **Manages server resources** with adaptive concurrency control
4. **Provides real-time feedback** via HTTP polling with granular progress updates
5. **Stores documents** in Supabase with vector embeddings for RAG
6. **Extracts code examples** automatically with AI-powered summaries
7. **Supports cancellation** for long-running operations
8. **Handles errors gracefully** with retry logic and detailed logging
9. **Scales intelligently** based on CPU, memory, and system load

### Key Files Reference

- **Entry Point**: `python/src/server/api_routes/knowledge_api.py`
- **Orchestrator**: `python/src/server/services/crawling/crawling_service.py`
- **Crawler Manager**: `python/src/server/services/crawler_manager.py`
- **Threading Service**: `python/src/server/services/threading_service.py`
- **Strategies**: `python/src/server/services/crawling/strategies/*.py`
- **URL Handling**: `python/src/server/services/crawling/helpers/url_handler.py`
- **Document Storage**: `python/src/server/services/crawling/document_storage_operations.py`
- **Progress Mapping**: `python/src/server/services/crawling/progress_mapper.py`

### External Dependencies

- **Crawl4AI v0.6.2**: Web crawling engine with Playwright
- **Supabase**: Vector database for document storage
- **OpenAI/Google/Ollama**: Embedding generation
- **FastAPI**: REST API framework
- **Pydantic**: Data validation
- **psutil**: System resource monitoring
- **asyncio**: Asynchronous I/O and concurrency control

