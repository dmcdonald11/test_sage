# =============================================================================
# OPENAI API CONFIGURATION
# =============================================================================
# Your OpenAI API key for agent communication
OPENAI_API_KEY=your_openai_api_key_here


# =============================================================================
# DOCLING TOKENIZER MODEL CONFIGURATION
# =============================================================================
# HuggingFace model to use for tokenization in the ExtractAndChunkWebsite tool
# All models listed below are FREE and OPEN SOURCE (Apache 2.0 or MIT License)
# Models are auto-downloaded from HuggingFace on first use and cached locally
# No API keys or accounts required
#
# Choose based on your needs:
# - Speed: Smaller models (MiniLM-L6) are faster
# - Quality: Larger models (MPNet, BERT) are more accurate
# - Language: Use multilingual models for non-English content
# - Domain: Use specialized models for code or scientific documents
# =============================================================================

TOKENIZER_MODEL=sentence-transformers/all-MiniLM-L6-v2


# -----------------------------------------------------------------------------
# RECOMMENDED MODELS (Sentence Transformers - Best for General Use)
# -----------------------------------------------------------------------------

# sentence-transformers/all-MiniLM-L6-v2 (DEFAULT - RECOMMENDED)
# - Size: Small (~80MB)
# - Speed: Very Fast
# - Dimensions: 384
# - Best for: General purpose chunking, balanced speed and quality
# - Language: English

# sentence-transformers/all-MiniLM-L12-v2
# - Size: Medium (~120MB)
# - Speed: Fast
# - Dimensions: 384
# - Best for: Better accuracy than L6, still fast
# - Language: English

# sentence-transformers/all-mpnet-base-v2
# - Size: Large (~420MB)
# - Speed: Moderate
# - Dimensions: 768
# - Best for: Highest quality embeddings, best semantic understanding
# - Language: English


# -----------------------------------------------------------------------------
# MULTILINGUAL MODELS (For Non-English Content)
# -----------------------------------------------------------------------------

# sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
# - Size: Medium (~470MB)
# - Speed: Fast
# - Dimensions: 384
# - Best for: 50+ languages, multilingual documents
# - Languages: Arabic, Chinese, Dutch, English, French, German, Italian, Korean, 
#              Polish, Portuguese, Russian, Spanish, Turkish, and 40+ more

# sentence-transformers/paraphrase-multilingual-mpnet-base-v2
# - Size: Large (~1GB)
# - Speed: Moderate
# - Dimensions: 768
# - Best for: Highest quality for multilingual content
# - Languages: 50+ languages

# xlm-roberta-base
# - Size: Large (~1GB)
# - Speed: Moderate to Slow
# - Dimensions: 768
# - Best for: Cross-lingual understanding, 100+ languages
# - Languages: 100+ languages

# bert-base-multilingual-cased
# - Size: Large (~680MB)
# - Speed: Moderate
# - Dimensions: 768
# - Best for: 104 languages with case sensitivity
# - Languages: 104 languages


# -----------------------------------------------------------------------------
# SPECIALIZED MODELS (Domain-Specific)
# -----------------------------------------------------------------------------

# microsoft/codebert-base
# - Size: Large (~500MB)
# - Speed: Moderate
# - Dimensions: 768
# - Best for: Code documentation, programming content, technical docs
# - Languages: Python, Java, JavaScript, PHP, Ruby, Go (6 languages)
# - Special: Trained on code repositories and docstrings

# allenai/scibert_scivocab_uncased
# - Size: Large (~440MB)
# - Speed: Moderate
# - Dimensions: 768
# - Best for: Scientific papers, research documents, academic content
# - Special: Trained on 1.14M scientific papers from Semantic Scholar

# sentence-transformers/multi-qa-MiniLM-L6-cos-v1
# - Size: Small (~80MB)
# - Speed: Very Fast
# - Dimensions: 384
# - Best for: Q&A documents, FAQ pages, conversational content
# - Special: Optimized for question-answer pair similarity


# -----------------------------------------------------------------------------
# STANDARD BERT MODELS (General Purpose)
# -----------------------------------------------------------------------------

# bert-base-uncased
# - Size: Large (~440MB)
# - Speed: Moderate
# - Dimensions: 768
# - Best for: Standard NLP tasks, general text
# - Language: English (lowercased)

# bert-base-cased
# - Size: Large (~440MB)
# - Speed: Moderate
# - Dimensions: 768
# - Best for: When case matters (proper nouns, abbreviations)
# - Language: English (case-sensitive)

# distilbert-base-uncased
# - Size: Medium (~260MB)
# - Speed: Fast
# - Dimensions: 768
# - Best for: Faster BERT alternative with 97% performance
# - Language: English

# roberta-base
# - Size: Large (~500MB)
# - Speed: Moderate
# - Dimensions: 768
# - Best for: Improved BERT variant, better on many tasks
# - Language: English


# -----------------------------------------------------------------------------
# PERFORMANCE COMPARISON
# -----------------------------------------------------------------------------
# Model                                          | Size    | Speed      | Quality | Use Case
# -----------------------------------------------|---------|------------|---------|------------------
# sentence-transformers/all-MiniLM-L6-v2        | Small   | Very Fast  | Good    | General (DEFAULT)
# sentence-transformers/all-MiniLM-L12-v2       | Medium  | Fast       | Better  | General
# sentence-transformers/all-mpnet-base-v2       | Large   | Moderate   | Best    | High Quality
# paraphrase-multilingual-MiniLM-L12-v2         | Medium  | Fast       | Good    | Multilingual
# microsoft/codebert-base                       | Large   | Moderate   | Best    | Code
# allenai/scibert_scivocab_uncased              | Large   | Moderate   | Best    | Science
# multi-qa-MiniLM-L6-cos-v1                     | Small   | Very Fast  | Good    | Q&A


# =============================================================================
# OTHER API KEYS (Add as needed)
# =============================================================================
# Add any other API keys your tools require below
# Example:
# SOME_OTHER_API_KEY=your_key_here
